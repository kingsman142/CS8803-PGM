\textbf{Problem 3}

The Ising model can be seen below, both the lattice and ``checkerboard'' are drawn. The checkerboard, as used as an example in the question, displays white tiles with label $w_i$ and black tiles with label $b_j$.

We want to show that $p(b_1, b_2, \dots \vert w_1, w_2, \dots) = p(b_1 \vert w_1, w_2, \dots)p(b_2 \vert w_1, w_2, \dots)\dots$ given that $p(x) \propto \exp(\beta\sum_{i\sim j} \mathbbm{1}[x_i = x_j])$ and the model is working on the basis of nearest-neighbor interactions.

Assuming $p(x)$ represents the probability of a node $X$ being in state $x$ with neighbors $Y$, then because of nearest-neighbour interactions, we have $p(x \vert Y) = \exp(\beta \sum_{y_j \in Y} \mathbbm{1}[x_i = y_j])$. 

As such, this general idea can be carried over to the joint probability of the black tiles conditioned on the white tiles. Since each black tile only interacts with its neighbors (i.e. $Y = {w_1, w_2, \dots}$), which are white, we can easily see none of the black tiles are neighbours of each other. Since none of the black tiles are neighbors of each other, they cannot interact each other (i.e. black tiles do not interact with any other black tiles). Since there are no black-black interactions, we can assume they are independent. Thus, in statistics terms, we know two variables are independent if $P(w, z) = P(w)P(z)$, or in other words, the two variables can be written as pdfs of themselves. A similar situation occurs here. Since for each black tile $X$, we have $Y$ that consists solely of white tiles, so each black tile can be written as a function of that one black tile along with all the white tiles. Basically, $P(b_1, b_2 \vert w_1, w_2, \dots) = P(b_1 \vert w_1, w_2, \dots)P(b_2 \vert w_1, w_2)$, similar to the example with $w$ and $z$ shown above. This can be similarly expanded to the general case: $p(b_1, b_2, \dots \vert w_1, w_2, \dots) = p(b_1 \vert w_1, w_2, \dots)p(b_2 \vert w_1, w_2, \dots)\dots$ $\qed$

This above idea can be exploited by the Gibbs sampling procedure. In Gibbs sampling, we want to estimate some values $(x_1, x_2, \dots, x_n)$. For each iteration, we select some random value $x_i$, and we want to make a guess/estimate for that variable's value, given the values of all the other values, or in other words we want to estimate $x_i$ from the distribution $p(x_i \vert x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = p(x_i \vert x_{-i})$. Now, want to relate this ideology back to the original nearest-neighbour checkerboard example in this question. We are trying to estimate the distribution $p(b_1, b_2, \dots \vert w_1, w_2, \dots)$. Since we know the black tiles are independent of each other given the white variables, then assuming we are given values of the white variables, we can easily estimate each black tile's distribution $p(b_i \vert w_1, w_2, \dots)$ given the remaining black tile variables. In other words, we can estimate $p(b_i \vert b_1, \dots b_{i-1}, b{i+1}, \dots, b_m)$ since we know the distribution of all the other black tiles ($p(b_j) \propto \exp(\beta \sum_{j \sim p} \mathbbm{1}[x_j = x_p])$). So, since we know the distribution of all the other black tiles, if we fix them (and assume they are givens), we can estimate the only non-fixed black tile's ($b_j$) value for that given iteration. So, clearly, since we can see the black variables are independent of each other given the white variables, we can abuse this knowledge to do Gibbs sampling.